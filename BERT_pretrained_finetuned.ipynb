{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is my pretrained and finetuned BERT for generation of words on a partially masked corpus of text i.e. Twitch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizing Hugging Face\n",
    "#https://huggingface.co/docs/transformers/quicktour\n",
    "\n",
    "#Refs\n",
    "#https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers datasets\n",
    "#%pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu${CUDA_VERSION}/torch_stable.html\n",
    "\n",
    "# %from transformers import EncoderDecoderModel, BertTokenizer\n",
    "#%pip freeze requirements.txt\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")    # use CPU\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding file: admiralbahroo.pkl\n",
      "Adding file: admiralbulldog.pkl\n"
     ]
    }
   ],
   "source": [
    "#Load Raw Data from StreamerData folder\n",
    "#- Take certain number of files from data folder to use for generation\n",
    "\n",
    "#Set this to the path of your data\n",
    "folder_path = 'C:/Users/anish/Projects/twitchBot/streamerdata'\n",
    "\n",
    "numFiles = 2\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, file in enumerate(os.listdir(folder_path)):\n",
    "    print(\"Adding file:\", file)\n",
    "    if i == numFiles - 1:\n",
    "        break  \n",
    "    with open(os.path.join(folder_path, file), 'rb') as f:\n",
    "        curr_file = pkl.load(f)\n",
    "        data.append(curr_file)\n",
    "\n",
    "all_df = pd.concat(data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>commenter_id</th>\n",
       "      <th>commenter_type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>fragments</th>\n",
       "      <th>offset</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STILL LATE STRIMMER IS A BAKA rooBaka STRIMMER...</td>\n",
       "      <td>40972890</td>\n",
       "      <td>110969182</td>\n",
       "      <td>user</td>\n",
       "      <td>2018-06-24T17:07:43.067Z</td>\n",
       "      <td>[{'text': 'STILL LATE STRIMMER IS A BAKA '}, {...</td>\n",
       "      <td>4.367</td>\n",
       "      <td>2018-06-24T17:07:43.067Z</td>\n",
       "      <td>277073045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOT LIVE rooREE rooREE rooREE</td>\n",
       "      <td>40972890</td>\n",
       "      <td>109856709</td>\n",
       "      <td>user</td>\n",
       "      <td>2018-06-24T17:07:43.168Z</td>\n",
       "      <td>[{'text': 'NOT LIVE '}, {'emoticon_id': '90487...</td>\n",
       "      <td>4.468</td>\n",
       "      <td>2018-06-24T17:07:43.168Z</td>\n",
       "      <td>277073045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rooBaka BAHROOrooBaka BAHROOrooBaka BAHROOrooB...</td>\n",
       "      <td>40972890</td>\n",
       "      <td>56159955</td>\n",
       "      <td>user</td>\n",
       "      <td>2018-06-24T17:07:44.631Z</td>\n",
       "      <td>[{'emoticon_id': '904818'}, {'text': ' BAHROOr...</td>\n",
       "      <td>5.931</td>\n",
       "      <td>2018-06-24T17:07:44.631Z</td>\n",
       "      <td>277073045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>See</td>\n",
       "      <td>40972890</td>\n",
       "      <td>78678476</td>\n",
       "      <td>user</td>\n",
       "      <td>2018-06-24T17:07:45.602Z</td>\n",
       "      <td>[{'text': 'See'}]</td>\n",
       "      <td>6.902</td>\n",
       "      <td>2018-06-24T17:07:45.602Z</td>\n",
       "      <td>277073045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rooPog</td>\n",
       "      <td>40972890</td>\n",
       "      <td>30898708</td>\n",
       "      <td>user</td>\n",
       "      <td>2018-06-24T17:07:45.802Z</td>\n",
       "      <td>[{'emoticon_id': '904866'}]</td>\n",
       "      <td>7.102</td>\n",
       "      <td>2018-06-24T17:07:45.802Z</td>\n",
       "      <td>277073045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body channel_id commenter_id  \\\n",
       "0  STILL LATE STRIMMER IS A BAKA rooBaka STRIMMER...   40972890    110969182   \n",
       "1                      NOT LIVE rooREE rooREE rooREE   40972890    109856709   \n",
       "2  rooBaka BAHROOrooBaka BAHROOrooBaka BAHROOrooB...   40972890     56159955   \n",
       "3                                                See   40972890     78678476   \n",
       "4                                             rooPog   40972890     30898708   \n",
       "\n",
       "  commenter_type                created_at  \\\n",
       "0           user  2018-06-24T17:07:43.067Z   \n",
       "1           user  2018-06-24T17:07:43.168Z   \n",
       "2           user  2018-06-24T17:07:44.631Z   \n",
       "3           user  2018-06-24T17:07:45.602Z   \n",
       "4           user  2018-06-24T17:07:45.802Z   \n",
       "\n",
       "                                           fragments  offset  \\\n",
       "0  [{'text': 'STILL LATE STRIMMER IS A BAKA '}, {...   4.367   \n",
       "1  [{'text': 'NOT LIVE '}, {'emoticon_id': '90487...   4.468   \n",
       "2  [{'emoticon_id': '904818'}, {'text': ' BAHROOr...   5.931   \n",
       "3                                  [{'text': 'See'}]   6.902   \n",
       "4                        [{'emoticon_id': '904866'}]   7.102   \n",
       "\n",
       "                 updated_at   video_id  \n",
       "0  2018-06-24T17:07:43.067Z  277073045  \n",
       "1  2018-06-24T17:07:43.168Z  277073045  \n",
       "2  2018-06-24T17:07:44.631Z  277073045  \n",
       "3  2018-06-24T17:07:45.602Z  277073045  \n",
       "4  2018-06-24T17:07:45.802Z  277073045  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['body', 'channel_id', 'commenter_id', 'commenter_type', 'created_at', 'fragments', 'offset', 'updated_at', 'video_id']\n",
      "Num Rows: 2722109\n",
      "Num Cols: 9\n"
     ]
    }
   ],
   "source": [
    "#Display all_data\n",
    "display(all_df.head())\n",
    "\n",
    "#Print data columns\n",
    "print([i for i in all_df.columns])\n",
    "\n",
    "#Print size of data\n",
    "print(\"Num Rows:\", len(all_df))\n",
    "print(\"Num Cols:\", len(all_df.columns))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [MASK] LATE STRIMMER IS A BAKA rooBaka STRIMME...\n",
       "Name: masked, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Filter Data Frame by Strings that are longer than 3 -> Allow for masking\n",
    "long_df = all_df[all_df['body'].apply(lambda x: len(x.split(' '))) > 3]\n",
    "\n",
    "#Use small dataframe for testing efficiency's sake\n",
    "long_df = long_df.iloc[:1]\n",
    "print(\"Number of Rows:\", len(long_df))\n",
    "\n",
    "#Replace certain words with Mask token '[MASK]'\n",
    "masked_arr = long_df.body.tolist()\n",
    "for i in range(len(masked_arr)):\n",
    "\n",
    "    add_mask = masked_arr[i].split(\" \")\n",
    "    mask_index = random.randint(0, len(add_mask) - 1)\n",
    "    add_mask[mask_index] = '[MASK]'\n",
    "    masked_arr[i] = \" \".join(add_mask)\n",
    "\n",
    "\n",
    "long_df['masked'] = masked_arr\n",
    "\n",
    "display(long_df['masked'].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Taking the processed data and tokenizing it for the BERT model\n",
    "#Using the bert-base-uncased model\"\n",
    "#- 110M params\n",
    "#- English\n",
    "#- Trained on BookCorpus, dataset consisting of 11,038 unpublished books and English Wikipedia\n",
    "#- Vocabulary size of 30,000\n",
    "#- inputs are of the form [CLS] Sentence A [SEP] Sentence B [SEP]\n",
    "\n",
    "\n",
    "model_pretrained = 'bert-base-uncased'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(model_pretrained)\n",
    "model = transformers.BertForMaskedLM.from_pretrained(model_pretrained, return_dict = True, output_hidden_states=True)\n",
    "long_df['tokens'] = [tokenizer.encode_plus(text, return_tensors = \"pt\") for text in long_df['masked']]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLMHead(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = torch.nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layer_norm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.decoder = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = self.dense(features)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.decoder(x) + self.bias\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        tokenized_text = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokenized_text['input_ids'], tokenized_text['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STILL LATE STRIMMER IS A BAKA rooBaka STRIMMER IS A BAKA rooBaka [MASK] IS A BAKA rooBaka']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(long_df['masked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   1194 MiB |   1194 MiB |   1741 MiB | 560650 KiB |\\n|       from large pool |   1184 MiB |   1184 MiB |   1731 MiB | 560640 KiB |\\n|       from small pool |     10 MiB |     10 MiB |     10 MiB |     10 KiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   1194 MiB |   1194 MiB |   1741 MiB | 560650 KiB |\\n|       from large pool |   1184 MiB |   1184 MiB |   1731 MiB | 560640 KiB |\\n|       from small pool |     10 MiB |     10 MiB |     10 MiB |     10 KiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   1174 MiB |   1174 MiB |   1702 MiB | 540682 KiB |\\n|       from large pool |   1164 MiB |   1164 MiB |   1692 MiB | 540672 KiB |\\n|       from small pool |     10 MiB |     10 MiB |     10 MiB |     10 KiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   1210 MiB |   1210 MiB |   1210 MiB |      0 B   |\\n|       from large pool |   1198 MiB |   1198 MiB |   1198 MiB |      0 B   |\\n|       from small pool |     12 MiB |     12 MiB |     12 MiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  16294 KiB |  54552 KiB |    956 MiB |    940 MiB |\\n|       from large pool |  14336 KiB |  52992 KiB |    946 MiB |    932 MiB |\\n|       from small pool |   1958 KiB |   2349 KiB |     10 MiB |      8 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     449    |     450    |     553    |     104    |\\n|       from large pool |     239    |     239    |     339    |     100    |\\n|       from small pool |     210    |     211    |     214    |       4    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     449    |     450    |     553    |     104    |\\n|       from large pool |     239    |     239    |     339    |     100    |\\n|       from small pool |     210    |     211    |     214    |       4    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      66    |      66    |      66    |       0    |\\n|       from large pool |      60    |      60    |      60    |       0    |\\n|       from small pool |       6    |       6    |       6    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       8    |      20    |     115    |     107    |\\n|       from large pool |       1    |      18    |     106    |     105    |\\n|       from small pool |       7    |       7    |       9    |       2    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 10.499448776245117\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 9.400875091552734\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 8.15539836883545\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 7.394267559051514\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 7.395198822021484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = TextDataset(list(long_df['masked']), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "mlm_head = MaskedLMHead(model.config).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# Define the scheduler\n",
    "num_training_steps = len(train_loader) * 3\n",
    "num_warmup_steps = int(num_training_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print('Epoch:', epoch+1)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids, attention_mask, *_  = batch\n",
    "        input_ids, attention_mask = input_ids.view(-1, input_ids.size(-1)), attention_mask.view(-1, attention_mask.size(-1))\n",
    "        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), labels=input_ids.to(device))\n",
    "        logits = mlm_head(outputs.hidden_states[-1]).to(device)\n",
    "        loss = criterion(logits.view(-1, logits.shape[-1]), input_ids.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print('Avg Loss:', avg_loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Masked: STILL LATE STRIMMER IS A BAKA rooBaka STRIMMER IS A BAKA rooBaka STRIMMER IS A BAKA rooBaka\n",
      "0 Predicted: the LATE STRIMMER IS A BAKA rooBaka STRIMMER IS A BAKA rooBaka STRIMMER IS A BAKA rooBaka\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "numberPredsPer = 10\n",
    "for text in long_df['masked']:\n",
    "    input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "    mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "    output = model(**input)\n",
    "    logits = output.logits\n",
    "    softmax = F.softmax(logits, dim = -1)\n",
    "    mask_word = softmax[0, mask_index, :]\n",
    "    top_10 = torch.topk(mask_word, numberPredsPer, dim = 1)[1][0]\n",
    "    curr_preds = []\n",
    "    for token in top_10:\n",
    "        word = tokenizer.decode([token])\n",
    "        new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "        curr_preds.append(new_sentence)\n",
    "    predictions.append(curr_preds)\n",
    "\n",
    "\n",
    "long_df['predictions'] = predictions\n",
    "\n",
    "#Display your Predictions vs. Actual!\n",
    "for row in range(len(long_df.body[:5])):\n",
    "    print(row, \"Masked:\", long_df.iloc[row].body)\n",
    "    print(row, \"Predicted:\", long_df.iloc[row].predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.9716494083404541\n"
     ]
    }
   ],
   "source": [
    "##Using Cosine Similarity as the metric of comparison\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return F.cosine_similarity(a, b, dim=1).item()\n",
    "\n",
    "#Calculate similarity over entire data set\n",
    "similarities = []\n",
    "for i in range(len(long_df.body)):\n",
    "    with torch.no_grad():\n",
    "        original_embedding = model(input_ids=long_df.iloc[i].tokens[\"input_ids\"]).hidden_states[-1].mean(dim=1)\n",
    "        predicted_embedding = model(input_ids=tokenizer.encode_plus(long_df.iloc[i].predictions[0], return_tensors='pt')['input_ids']).hidden_states[-1].mean(dim=1)\n",
    "\n",
    "    similarity = cosine_similarity(original_embedding, predicted_embedding)\n",
    "    similarities.append(similarity)\n",
    "print(\"Cosine similarity:\", sum(similarities) / len(similarities))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Alternate Methodolgy of Calling Hugging Face Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"The quick brown fox jumps over the lazy [MASK].\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
